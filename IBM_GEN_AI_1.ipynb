{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-bRPE7NxL3I",
        "outputId": "641b4dce-9741-464a-8a16-e807a80d82cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Prompt 1: Greedy Decoding -----\n",
            "Token IDs: [464, 6156, 9379, 6364, 4721, 663, 2951, 329, 262, 717, 640, 13, 198, 198, 1, 2061, 338, 2642, 1701, 198, 198, 1, 40, 1101, 407, 1654, 13, 314, 1101, 407, 1654, 644, 338, 2642, 13, 314, 1101, 407, 1654, 644, 338, 2642, 13, 314, 1101, 407, 1654, 644, 338, 2642]\n",
            "Decoded Text: The ancient robot slowly opened its eyes for the first time.\n",
            "\n",
            "\"What's wrong?\"\n",
            "\n",
            "\"I'm not sure. I'm not sure what's wrong. I'm not sure what's wrong. I'm not sure what's wrong\n",
            "\n",
            "----- Prompt 2: Greedy Decoding -----\n",
            "Token IDs: [818, 262, 2612, 286, 262, 42282, 8222, 11, 257, 31992, 22211, 13, 198, 198, 1, 40, 1101, 407, 1654, 611, 345, 821, 826, 11, 475, 314, 1101, 1654, 345, 821, 826, 13, 314, 1101, 407, 1654, 611, 345, 821, 826, 11, 475, 314, 1101, 1654, 345, 821, 826, 13, 314]\n",
            "Decoded Text: In the heart of the enchanted forest, a whisper echoed.\n",
            "\n",
            "\"I'm not sure if you're right, but I'm sure you're right. I'm not sure if you're right, but I'm sure you're right. I\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Prompt 1: Temperature Sampling (temp=0.7) -----\n",
            "The ancient robot slowly opened its eyes for the first time. It was a small robot with a red and yellow body. It was an orange robot with a black and green face and a red head and its eyes were closed.\n",
            "\n",
            "\"What's\n",
            "\n",
            "----- Prompt 1: Top-k Sampling (k=50) -----\n",
            "The ancient robot slowly opened its eyes for the first time.\n",
            "\n",
            "The robot began humming loudly. \"Ooh, hello.\"\n",
            "\n",
            "\"Hey! What do you want?\" The robot replied. The robot was surprised. \"Have you been to\n",
            "\n",
            "----- Prompt 1: Top-p Sampling (p=0.9) -----\n",
            "The ancient robot slowly opened its eyes for the first time.\n",
            "\n",
            "A small voice came from inside the robot's mind.\n",
            "\n",
            "That voice was something that could never be revealed.\n",
            "\n",
            "\"…Oh come now. I'm telling you so and\n",
            "\n",
            "Analysis\n",
            "\n",
            "Q6: Compare the outputs from greedy decoding and the three sampling techniques:\n",
            "\n",
            "- Greedy decoding is safe and usually makes sense, but it’s predictable.\n",
            "- Temperature sampling with 0.7 added some randomness and gave slightly more engaging text.\n",
            "- Top-k sampling (k=50) had more creative and poetic lines, though sometimes a bit random.\n",
            "- Top-p sampling (p=0.9) was the most balanced — it felt meaningful and also creative.\n",
            "\n",
            "Most coherent: Greedy and Temperature\n",
            "Most creative: Top-p\n",
            "Most surprising: Top-k\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "prompt1 = \"The ancient robot slowly opened its eyes for the first time.\"\n",
        "prompt2 = \"In the heart of the enchanted forest, a whisper echoed.\"\n",
        "\n",
        "input_ids1 = tokenizer.encode(prompt1, return_tensors=\"pt\")\n",
        "input_ids2 = tokenizer.encode(prompt2, return_tensors=\"pt\")\n",
        "\n",
        "output_greedy1 = model.generate(input_ids1, max_length=50)\n",
        "output_greedy2 = model.generate(input_ids2, max_length=50)\n",
        "\n",
        "print(\"----- Prompt 1: Greedy Decoding -----\")\n",
        "print(\"Token IDs:\", output_greedy1[0].tolist())\n",
        "print(\"Decoded Text:\", tokenizer.decode(output_greedy1[0], skip_special_tokens=True))\n",
        "print()\n",
        "\n",
        "print(\"----- Prompt 2: Greedy Decoding -----\")\n",
        "print(\"Token IDs:\", output_greedy2[0].tolist())\n",
        "print(\"Decoded Text:\", tokenizer.decode(output_greedy2[0], skip_special_tokens=True))\n",
        "print()\n",
        "\n",
        "output_temp = model.generate(\n",
        "    input_ids1,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    max_length=50\n",
        ")\n",
        "\n",
        "output_topk = model.generate(\n",
        "    input_ids1,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    max_length=50\n",
        ")\n",
        "\n",
        "output_topp = model.generate(\n",
        "    input_ids1,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    max_length=50\n",
        ")\n",
        "\n",
        "print(\"----- Prompt 1: Temperature Sampling (temp=0.7) -----\")\n",
        "print(tokenizer.decode(output_temp[0], skip_special_tokens=True))\n",
        "print()\n",
        "\n",
        "print(\"----- Prompt 1: Top-k Sampling (k=50) -----\")\n",
        "print(tokenizer.decode(output_topk[0], skip_special_tokens=True))\n",
        "print()\n",
        "\n",
        "print(\"----- Prompt 1: Top-p Sampling (p=0.9) -----\")\n",
        "print(tokenizer.decode(output_topp[0], skip_special_tokens=True))\n",
        "print()\n",
        "\n",
        "print(\"Analysis\")\n",
        "\n",
        "print(\"\"\"\n",
        "Q6: Compare the outputs from greedy decoding and the three sampling techniques:\n",
        "\n",
        "- Greedy decoding is safe and usually makes sense, but it’s predictable.\n",
        "- Temperature sampling with 0.7 added some randomness and gave slightly more engaging text.\n",
        "- Top-k sampling (k=50) had more creative and poetic lines, though sometimes a bit random.\n",
        "- Top-p sampling (p=0.9) was the most balanced — it felt meaningful and also creative.\n",
        "\n",
        "Most coherent: Greedy and Temperature\n",
        "Most creative: Top-p\n",
        "Most surprising: Top-k\n",
        "\"\"\")\n"
      ]
    }
  ]
}